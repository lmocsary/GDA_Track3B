{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Preparation",
   "metadata": {
    "id": "eLMV7W2SpmS_",
    "cell_id": "00000-6c77cdb8-70a1-4f4b-b657-3b988edcd917",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-1796f838-ba18-463c-b030-a839047172f6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "93cde8f1",
    "execution_start": 1638528431522,
    "execution_millis": 8551,
    "is_output_hidden": true,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install tensorflow_datasets",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting tensorflow_datasets\n  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n\u001b[K     |████████████████████████████████| 4.0 MB 25.9 MB/s \n\u001b[?25hRequirement already satisfied: numpy in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_datasets) (1.19.5)\nRequirement already satisfied: future in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_datasets) (0.18.2)\nRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from tensorflow_datasets) (3.10.0.2)\nCollecting tensorflow-metadata\n  Downloading tensorflow_metadata-1.5.0-py3-none-any.whl (48 kB)\n\u001b[K     |████████████████████████████████| 48 kB 10.1 MB/s \n\u001b[?25hRequirement already satisfied: tqdm in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_datasets) (4.62.3)\nRequirement already satisfied: six in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from tensorflow_datasets) (1.16.0)\nRequirement already satisfied: termcolor in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_datasets) (1.1.0)\nRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_datasets) (5.4.0)\nRequirement already satisfied: dill in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_datasets) (0.3.4)\nRequirement already satisfied: absl-py in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_datasets) (0.15.0)\nRequirement already satisfied: requests>=2.19.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_datasets) (2.26.0)\nCollecting promise\n  Downloading promise-2.3.tar.gz (19 kB)\nRequirement already satisfied: protobuf>=3.12.2 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_datasets) (3.19.1)\nRequirement already satisfied: attrs>=18.1.0 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from tensorflow_datasets) (21.2.0)\nRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\nRequirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.6.0)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\nBuilding wheels for collected packages: promise\n  Building wheel for promise (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21493 sha256=1a3bdd13f01990e4861f95597cb8c4a1b1d132458999ccf41c8a856ba3229079\n  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\nSuccessfully built promise\n\u001b[31mERROR: tensorflow-metadata 1.5.0 has requirement absl-py<0.13,>=0.9, but you'll have absl-py 0.15.0 which is incompatible.\u001b[0m\nInstalling collected packages: tensorflow-metadata, promise, tensorflow-datasets\nSuccessfully installed promise-2.3 tensorflow-datasets-4.4.0 tensorflow-metadata-1.5.0\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nh3aDkAmUPMe",
    "cell_id": "00001-9e61b5cc-3e53-4158-aa48-fe0f31c7f738",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1f3d3b8a",
    "execution_start": 1638528440075,
    "execution_millis": 3642,
    "deepnote_cell_type": "code"
   },
   "source": "import tensorflow as tf\n\nfrom tensorflow.keras import datasets, layers, models\nimport tensorflow_datasets as tfds\n\nimport matplotlib.pyplot as plt",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vZT5_O_l0bqT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "outputId": "330c7255-fa2a-4295-813b-70df5eaaadc3",
    "cell_id": "00002-872f5946-32e0-45e1-a07e-959032d23ed2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "623fc1cb",
    "execution_start": 1638528443722,
    "execution_millis": 5554,
    "deepnote_cell_type": "code"
   },
   "source": "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 3s 0us/step\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rgtzfj5f0cfG",
    "cell_id": "00003-d29805dc-4b2f-4c23-b2c6-2a50233dbb1b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4ab31d42",
    "execution_start": 1638528449280,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "In this quest, you will work with the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). It contains 60000 32x32 colour images in 10 classes, with 6000 images per class. Your task will be predicting image classes.\n",
   "metadata": {
    "id": "9McRkqNvscOz",
    "cell_id": "00004-069ba11e-0bd2-4c30-bdab-f233d34838ff",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Tasks",
   "metadata": {
    "id": "gs2BVhIgsKCD",
    "cell_id": "00005-aa0866e6-a7a4-432e-81ba-3610dcd362cd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Normalize pixel values of both train and test images to the values in the range between 0 and 1",
   "metadata": {
    "id": "mp1MM9_DsuhS",
    "cell_id": "00006-ec1e8fe1-132d-47e1-9814-5c06091e06fa",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lZBGvhl0sDWI",
    "cell_id": "00007-d4b5b0ef-e29f-4c47-ad9e-ce548e17136b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d372e835",
    "execution_start": 1638528449287,
    "execution_millis": 1479,
    "deepnote_output_heights": [
     20.5
    ],
    "deepnote_cell_type": "code"
   },
   "source": "# your code\ntrain_images_normed = train_images/255.0\ntest_images_normed = test_images/255.0\n\ntest_images_normed.shape, test_labels.shape",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 6,
     "data": {
      "text/plain": "((10000, 32, 32, 3), (10000, 1))"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "labels = list(set((train_labels).flatten()))\nprint(labels)",
   "metadata": {
    "tags": [],
    "cell_id": "00010-30b89345-7948-41c1-9f42-b6d1f20c5b39",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f9314121",
    "execution_start": 1638530323205,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-89a802d2-fb31-4a3c-9e98-771a600c1aac",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "660f7fbf",
    "execution_start": 1638530821928,
    "execution_millis": 144,
    "deepnote_output_heights": [
     157.984375
    ],
    "deepnote_cell_type": "code"
   },
   "source": "import random\nn_im = random.randint(0,train_labels.shape[0])\nplt.figure(figsize=(2,2))\nplt.imshow(train_images[n_im], vmin = 0, vmax = 255, )\n#plt.colorbar()\nplt.grid(False)\nplt.title(class_names[int(train_labels[n_im])])\nplt.show()",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 144x144 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light",
      "image/png": {
       "width": 142,
       "height": 156
      }
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-77290478-c108-40db-8b27-233c0ffb9c7c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d7556b6a",
    "execution_start": 1638530834886,
    "execution_millis": 3,
    "deepnote_cell_type": "code"
   },
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D",
   "execution_count": 109,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Build a sequential model with the following architecture:\n\nconv2d - (None, 30, 30, 32)  \n_________________________________________________________________\nmax_pooling2d - (None, 15, 15, 32)        \n_________________________________________________________________\nconv2d - (None, 13, 13, 64)\n_________________________________________________________________\nmax_pooling2d - (None, 6, 6, 64)        \n_________________________________________________________________\nconv2d - (None, 4, 4, 128) \n_________________________________________________________________\nflatten - (None, 2048)       \n_________________________________________________________________\ndense - (None, 64)    \n_________________________________________________________________\ndense - (None, 10)       \n",
   "metadata": {
    "id": "aCaySZ8Ss_aj",
    "cell_id": "00008-271fd394-ffe0-4642-8211-6aeb84c88b8b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RFsiddt20yLp",
    "cell_id": "00009-13cc13f1-c6a8-4163-811d-7497e6488aa5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e78d89cd",
    "execution_start": 1638530846367,
    "execution_millis": 130,
    "deepnote_cell_type": "code"
   },
   "source": "# your code\n#create model\nmodel_cnn = Sequential()\n#add model layers\nnb_filters_first_cnn = 32\nnb_filters_second_cnn = 64\nnb_filters_third_cnn = 128\nnb_out_layer_neurons = len(labels) # defined by number of classes in classification task \nfilter_size = 3\n\n#32 filters ,each filter has size 3, if kernel size specified that way, 3 is interpreted as 3*3\nmodel_cnn.add(Conv2D(nb_filters_first_cnn, kernel_size=filter_size, activation=\"relu\", input_shape=(32,32,3)))\n# kernel size: specifying the height and width of the 2D convolution filter. \n# Can be a single integer to specify the same value for all spatial dimensions\n\nmodel_cnn.add(MaxPooling2D(pool_size=(2,2)))\nmodel_cnn.add(Conv2D(nb_filters_second_cnn, kernel_size=filter_size, activation=\"relu\"))\nmodel_cnn.add(MaxPooling2D(pool_size=(2,2)))\nmodel_cnn.add(Conv2D(nb_filters_third_cnn, kernel_size=filter_size, activation=\"relu\"))\n\n# 3d datastructure from output of prev conv layer, will flatten into 1dim vector\nmodel_cnn.add(Flatten())\n\nmodel_cnn.add(Dense(64, activation=\"relu\"))\nmodel_cnn.add(Dense(nb_out_layer_neurons, activation=\"softmax\"))\nmodel_cnn.summary()",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 30, 30, 32)        896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 4, 4, 128)         73856     \n_________________________________________________________________\nflatten (Flatten)            (None, 2048)              0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                131136    \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                650       \n=================================================================\nTotal params: 225,034\nTrainable params: 225,034\nNon-trainable params: 0\n_________________________________________________________________\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Compile the model using Adam optimizer, sparse categorical crossentropy as loss function, and choose an appropriate metric for classification. Use 10 epochs to train the model.",
   "metadata": {
    "id": "2blKJv2huDPh",
    "cell_id": "00010-5d3e9bc0-d321-40d8-a841-bd7b8d911be9",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "train_images_normed.shape, test_images_normed.shape",
   "metadata": {
    "tags": [],
    "cell_id": "00015-f74badb3-cb4b-4392-be53-73358d6bc58a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "56db3739",
    "execution_start": 1638530878437,
    "execution_millis": 1,
    "deepnote_output_heights": [
     20.5
    ],
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 111,
     "data": {
      "text/plain": "((50000, 32, 32, 3), (10000, 32, 32, 3))"
     },
     "metadata": {}
    }
   ],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "source": "train_labels.shape, test_labels.shape",
   "metadata": {
    "tags": [],
    "cell_id": "00016-2f964d84-24cd-40b6-b6d0-725be08b1db8",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e3ff7368",
    "execution_start": 1638530881750,
    "execution_millis": 4,
    "deepnote_output_heights": [
     20.5
    ],
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 112,
     "data": {
      "text/plain": "((50000, 1), (10000, 1))"
     },
     "metadata": {}
    }
   ],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "source": "# one-hot encode the training and testing labels\n# in case of one-hot-encoded labels do not use sparse_categorical_entropy as loss but use categorical_entropy!\n\nfrom tensorflow.keras.utils import to_categorical\n\n#train_labels = to_categorical(train_labels, 10)\n#test_labels = to_categorical(test_labels, 10)",
   "metadata": {
    "tags": [],
    "cell_id": "00016-742df283-073f-4c87-ba0a-e36cf3c3cc67",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c432bb6c",
    "execution_start": 1638530901836,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "source": "train_labels.shape, test_labels.shape",
   "metadata": {
    "tags": [],
    "cell_id": "00017-8225d725-f9da-448c-a325-a8e0f455b261",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e3ff7368",
    "deepnote_output_heights": [
     20.5
    ],
    "allow_embed": false,
    "execution_start": 1638530906181,
    "execution_millis": 119,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 114,
     "data": {
      "text/plain": "((50000, 1), (10000, 1))"
     },
     "metadata": {}
    }
   ],
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "flXnIRIG09nO",
    "cell_id": "00011-7e2d47f7-7fc9-4cd4-8b64-0f0d14a54f9c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1c46462e",
    "execution_start": 1638517129623,
    "execution_millis": 1515762,
    "deepnote_cell_type": "code"
   },
   "source": "# your code\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\n\nmodel_cnn.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(0.002),\n                  metrics=['accuracy'])\n\nstory = model_cnn.fit(train_images_normed, train_labels, epochs=15, batch_size=100, validation_split=0.2  )                  ",
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/15\n400/400 [==============================] - 124s 308ms/step - loss: 1.9363 - accuracy: 0.2620 - val_loss: 1.4344 - val_accuracy: 0.4858\nEpoch 2/15\n400/400 [==============================] - 117s 291ms/step - loss: 1.2991 - accuracy: 0.5294 - val_loss: 1.1693 - val_accuracy: 0.5917\nEpoch 3/15\n400/400 [==============================] - 122s 305ms/step - loss: 1.0955 - accuracy: 0.6085 - val_loss: 1.0505 - val_accuracy: 0.6306\nEpoch 4/15\n400/400 [==============================] - 113s 282ms/step - loss: 0.9721 - accuracy: 0.6554 - val_loss: 0.9837 - val_accuracy: 0.6570\nEpoch 5/15\n400/400 [==============================] - 108s 269ms/step - loss: 0.8867 - accuracy: 0.6885 - val_loss: 0.9265 - val_accuracy: 0.6748\nEpoch 6/15\n400/400 [==============================] - 85s 213ms/step - loss: 0.8020 - accuracy: 0.7215 - val_loss: 0.9001 - val_accuracy: 0.6942\nEpoch 7/15\n400/400 [==============================] - 85s 212ms/step - loss: 0.7312 - accuracy: 0.7423 - val_loss: 0.8609 - val_accuracy: 0.7028\nEpoch 8/15\n400/400 [==============================] - 88s 219ms/step - loss: 0.6911 - accuracy: 0.7569 - val_loss: 0.8666 - val_accuracy: 0.7045\nEpoch 9/15\n400/400 [==============================] - 104s 259ms/step - loss: 0.6311 - accuracy: 0.7801 - val_loss: 0.8425 - val_accuracy: 0.7112\nEpoch 10/15\n400/400 [==============================] - 88s 221ms/step - loss: 0.5933 - accuracy: 0.7949 - val_loss: 0.8875 - val_accuracy: 0.7097\nEpoch 11/15\n400/400 [==============================] - 102s 255ms/step - loss: 0.5658 - accuracy: 0.8004 - val_loss: 0.9022 - val_accuracy: 0.7049\nEpoch 12/15\n400/400 [==============================] - 84s 211ms/step - loss: 0.5364 - accuracy: 0.8117 - val_loss: 0.9107 - val_accuracy: 0.7015\nEpoch 13/15\n400/400 [==============================] - 86s 215ms/step - loss: 0.4928 - accuracy: 0.8281 - val_loss: 0.9250 - val_accuracy: 0.7112\nEpoch 14/15\n400/400 [==============================] - 101s 252ms/step - loss: 0.4651 - accuracy: 0.8384 - val_loss: 0.9460 - val_accuracy: 0.7058\nEpoch 15/15\n400/400 [==============================] - 109s 271ms/step - loss: 0.4441 - accuracy: 0.8442 - val_loss: 0.9963 - val_accuracy: 0.7001\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Evaluate the model on test data to get the loss and accuracy metrics",
   "metadata": {
    "id": "tyfR1bCiucDE",
    "cell_id": "00012-f92a8fa1-0809-49d6-a05e-26122dfb2f0c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CA4Hsi6B035n",
    "cell_id": "00013-61a53f9b-8086-47d3-bdb5-b2216b0f902c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "39b587d3",
    "execution_start": 1638518645395,
    "execution_millis": 7600,
    "deepnote_cell_type": "code"
   },
   "source": "# your code\nprint(f'Loss and accuracy:\\n {model_cnn.evaluate(test_images_normed, test_labels)}')\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "313/313 [==============================] - 7s 23ms/step - loss: 1.0533 - accuracy: 0.6951\nLoss and accuracy:\n [1.0532894134521484, 0.6951000094413757]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a109ea11-a7da-42aa-8af0-3a0cdafc917e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Quest CNN Classification v2.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "deepnote_notebook_id": "938cf550-6c0d-4a83-bf86-3c1525f57aa7",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}