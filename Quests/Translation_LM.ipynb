{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAgbCF4OFgv9"
      },
      "source": [
        "### Introduction\n",
        "In this exercise you will build three (optionally four) different networks to experiment with one of the applications of recurrent neural networks called Machine Translation. The preprocessing pipeline that prepares the input data for feeding into the neural network models is provided to you. Therefore, in this excercise you focus on building and experimenting with different network architectures. \n",
        "\n",
        "Optional: To learn more about recurrent neural networks (such as RNN, GRU, and LSTM) their architectures, the differences between them and their applications you can watch this free tutorial: [Sequences and Recurrent networks Tutorial](https://www.youtube.com/watch?v=87kLfzmYBy8&ab_channel=DeepMind)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGrOSRpUxG9L"
      },
      "source": [
        "# You may want to upgrade the tensorflow package to the latest verion (currently version 2.3) if you have the proper infrastructure\n",
        "# !pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2j9mYAX4o5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38aca040-5413-432c-9d02-3f20fe974137"
      },
      "source": [
        "pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.41.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.21.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMzRkuZbgt3k"
      },
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fobJjbQhBuju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0917a481-a487-4b29-b90f-c47b2d3cb974"
      },
      "source": [
        "# To get access to a GPU instance you can use the `change runtime type` and set the option to `GPU` from the `Runtime` tab  in the notebook\n",
        "# Checking the GPU availability for the notebook\n",
        "import tensorflow\n",
        "tensorflow.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD0z5tl4dVOM"
      },
      "source": [
        "# import collection\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu1mXD8unuu9"
      },
      "source": [
        "### Loading and Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPqbg_C09G6s"
      },
      "source": [
        "#### Reading CSV data files as a list of strings, where each string represents a single line from the text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHuc68kPoSqo"
      },
      "source": [
        "import csv\n",
        "from io import TextIOWrapper\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_Kh3XXt2GGb",
        "outputId": "f5c31e6d-3a83-4310-bd51-95aa72e2bfb9"
      },
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "url = 'https://github.com/murpi/wilddata/blob/master/quests/small_vocab_en.csv.zip?raw=true'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "\n",
        "open('small_vocab_en.csv.zip', 'wb').write(r.content)\n",
        "\n",
        "url = 'https://github.com/murpi/wilddata/blob/master/quests/small_vocab_fr.csv.zip?raw=true'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "\n",
        "open('small_vocab_fr.csv.zip', 'wb').write(r.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1367852"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGBtz_Lt39XM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80489b7b-0055-4594-bc2e-4ac4145ff177"
      },
      "source": [
        "with ZipFile('small_vocab_en.csv.zip') as zf:\n",
        "    with zf.open('small_vocab_en.csv', 'r') as infile:\n",
        "        reader = csv.reader(TextIOWrapper(infile, 'utf-8'), delimiter='\\n' )\n",
        "        english_text = [item for sublist in reader for item in sublist]\n",
        "        print(f\"English text samples: {english_text[0:2]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English text samples: ['new jersey is sometimes quiet during autumn , and it is snowy in april .', 'the united states is usually chilly during july , and it is usually freezing in november .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vVS_nH-4y6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e51ae4d-c603-4091-a22b-6fb5f1d4ad81"
      },
      "source": [
        "with ZipFile('small_vocab_fr.csv.zip') as zf:\n",
        "    with zf.open('small_vocab_fr.csv', 'r') as infile:\n",
        "        reader = csv.reader(TextIOWrapper(infile, 'utf-8'), delimiter='\\n' )\n",
        "        french_text = [item for sublist in reader for item in sublist]\n",
        "        print(f\"French translation text samples: {french_text[0:2]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French translation text samples: [\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\", 'les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5y0rPhaH66T"
      },
      "source": [
        "# TODO: Data exploration -> calculate and print some statistics on data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ8zEtacK08P"
      },
      "source": [
        "#### Tokenizing the data\n",
        "\n",
        "Since the input data to neural networks needs to be in numerical format, we first turn each sentence into a sequence of word ids using `Tokenizer` function from Keras. Word ids are numerical presentations for words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6s5iwnMKzT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75cb7311-9d7e-4a33-cb54-a3451b2105d4"
      },
      "source": [
        "def tokenize(x: List[str]):\n",
        "  \"\"\"Tokenizes sentences into word ids.\n",
        "    Tokenize x\n",
        "    :param x: List of sentences/strings to be tokenized\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
        "\n",
        "  \"\"\"\n",
        "  tokenizer=Tokenizer()\n",
        "  tokenizer.fit_on_texts(x)\n",
        "  t=tokenizer.texts_to_sequences(x)\n",
        "  return t, tokenizer\n",
        "\n",
        "# Tokenize Example output\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
            "Sequence 1 in x\n",
            "  Input:  The quick brown fox jumps over the lazy dog .\n",
            "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
            "Sequence 2 in x\n",
            "  Input:  By Jove , my quick study of lexicography won a prize .\n",
            "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
            "Sequence 3 in x\n",
            "  Input:  This is a short sentence .\n",
            "  Output: [18, 19, 3, 20, 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfffhgywsXMN"
      },
      "source": [
        "#### Padding Sequences\n",
        "\n",
        "To be able to batch the sequences of word ids together, all sequences need to be of the same length. Therefore, as a second preprocessing step we make sure that all the English sentences have the same length as their respective French translations using `pad_sequence` function form Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsB7MNttVYVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fb6cda-e522-41d7-e5b9-9e5d301ce485"
      },
      "source": [
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "    padding=pad_sequences(x,padding='post',maxlen=length)\n",
        "    return padding\n",
        "\n",
        "# Pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [1 2 4 5 6 7 1 8 9]\n",
            "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
            "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
            "Sequence 3 in x\n",
            "  Input:  [18 19  3 20 21]\n",
            "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzZF_s8TuGqw"
      },
      "source": [
        "#### Preprocessing Pipeline\n",
        "\n",
        "As mentioned in the introduction section in this excercise your focus is on building different recurrent neural network architectures. Threfore, in the following cell we provide you with a preprocessing pipeline. You can apply this function to the input data to prepare the data for using in neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B158-HbGqMwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941fc699-b8b6-4c2c-b04f-0e4fbaa7ed75"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "     Preprocess input (x) and target (y)\n",
        "    :param x: Feature List of sentences\n",
        "    :param y: Label List of sentences\n",
        "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_text, french_text)\n",
        "    \n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Preprocessed Data Info')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Data Info\n",
            "Max English sentence length: 15\n",
            "Max French sentence length: 21\n",
            "English vocabulary size: 199\n",
            "French vocabulary size: 344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_fCUxliHXwO"
      },
      "source": [
        "### Models\n",
        "\n",
        "In this section we ask you to build and train Three (optionally four) different neural networks and use them to predict the French Translation of the English sentences.\n",
        "\n",
        "Make use of the `logits_to_text()` function to transform the logits from output of your networks to a French translation. This helps you to better understand the output of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CrRvXm91nhd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdbd2a6b-39e0-460a-8667-f6ba9aa79aa4"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    \"\"\"\n",
        "    Turn logits from a neural network into text using the tokenizer\n",
        "    :param logits: Logits from a neural network\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "print('`logits_to_text` function loaded.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`logits_to_text` function loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhYC7lJmxpdd"
      },
      "source": [
        "#### Model 1: Implement a simple RNN network and train the network. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcO7xkUlJyU"
      },
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Input,InputLayer, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQB93125ldUo"
      },
      "source": [
        "# Model 1: simple RNN model\n",
        "EarlyStoping = EarlyStopping(monitor=\"val_loss\",verbose=1,mode='min',patience=3)\n",
        "\n",
        "\n",
        "def rnn_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a basic RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Build the layers and compile the model. (Hint: Use a GRU network followed by a TimeDistributed dense layer from tensorflow.keras.layers module)\n",
        "    # What is the input shape of your model? Which activation function do you choose for the dense layer?\n",
        "    \n",
        "    model = Sequential( # Complete here\n",
        "        \n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "model = rnn_model(preproc_english_sentences_21.shape,max_french_sequence_length,english_vocab_size+1,french_vocab_size+1)\n",
        "\n",
        "model.compile(loss=sparse_categorical_crossentropy,\n",
        "              optimizer=Adam(0.005),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# TODO: Preprocess and reshape the input to work with a basic RNN. (Hint: Use preproc_english_sentences, preproc_french_sentences and reshape function from keras)\n",
        "\n",
        "# TODO: Train the network you built in rnn_model. What size do you choose for the epochs, batch_size and validation_split?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7nwogKT2Y2Lp",
        "outputId": "5cbb73b2-2c4a-4f2a-f79c-ab5338e179af"
      },
      "source": [
        "# TODO: Print prediction(s) for one or more of the English sentences. Hint: Make use of the `logits_to_text` function for transforming output logits to text. \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'les états unis est généralement froid en septembre et il est généralement agréable en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQacJtFZ3Zic"
      },
      "source": [
        "#### Model 2: Implement an RNN model using word embeddings\n",
        "\n",
        "Embeddings are better representation of words. An embedding is a vector representation of a word in n-dimensional space, where n represents the size of the embedding vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ropzm7KD8juF"
      },
      "source": [
        "# Model 2: Embedding\n",
        "EarlyStoping = EarlyStopping(monitor=\"val_loss\",verbose=1,mode='min',patience=3)\n",
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Build the layers and compile the model. (Hint: Similar to the first model use a GRU network followed by a TimeDistributed dense layer from tensorflow.keras.layers module. \n",
        "    # And this time make use of Embedding as well!)\n",
        "    # What is the input shape of your model? Which activation function do you choose for the dense layer?\n",
        "    embedding_size = 128  # a common choice but fully arbitrary chosen here \n",
        "    model = Sequential( # Complete here\n",
        "                       \n",
        "\n",
        "                       \n",
        "    return model\n",
        "\n",
        "model_embeding = embed_model(preproc_english_sentences_21.shape,max_french_sequence_length,english_vocab_size+1,french_vocab_size+1) \n",
        "\n",
        "histemb=model_embeding.fit(preproc_english_sentences_21,preproc_french_sentences,validation_split=0.2, epochs=100, verbose=1,callbacks=[EarlyStoping])\n",
        "\n",
        "# TODO: Preprocess and reshape the input to work with a basic RNN. (Hint: Use preproc_english_sentences, preproc_french_sentences and reshape function from keras)\n",
        " \n",
        "# TODO: Train the network you built in rnn_model. What size do you choose for the epochs, batch_size and validation_split?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhmKgzbjbrkx"
      },
      "source": [
        "# TODO: Print prediction(s) for one or more of the English sentences. Hint: Make use of the `logits_to_text` function for transforming output logits to text. \n",
        "# (Hint: If you predict on the same sentence you can compare the result of different model on that specific sentence with each other.)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqMHh7Hw3X_z"
      },
      "source": [
        "#### Model 3: Implement a Bidirectional RNN network.\n",
        "\n",
        "One of the restrictions of RNN networks is that they only see the data from past. However, Bidirectional RNNs can see the data in both directions past and future. Make use of Bidirectionality and experiment if this feature enhances your network's performance or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_ZqZ7PqEWSM"
      },
      "source": [
        "# Model 3: Bidirectional RNNs\n",
        "EarlyStoping = EarlyStopping(monitor=\"val_loss\",verbose=1,mode='min',patience=3)\n",
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a bidirectional RNN model on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO:  Build the layers and compile the model. (Hint: Similar to the first model use a GRU network followed by a TimeDistributed dense layer from tensorflow.keras.layers module. \n",
        "    # Don't forget to make your network bidirectional!)\n",
        "\n",
        "    model = Sequential( # Complete here\n",
        "                       \n",
        "\n",
        "    return model\n",
        "model_bidirectional = bd_model(preproc_english_sentences_21.shape,max_french_sequence_length,english_vocab_size+1,french_vocab_size+1) \n",
        "\n",
        "histbd=model_bidirectional.fit(preproc_english_sentences_21,preproc_french_sentences,validation_split=0.2, epochs=100, verbose=1,callbacks=[EarlyStoping])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Q2AVwqeCUP"
      },
      "source": [
        "# TODO: Print prediction(s) for one or more of the English sentences. Hint: Make use of the `logits_to_text` function for transforming output logits to text.\n",
        "# (Hint: If you predict on the same sentence you can compare the result of different model on that specific sentence with each other.)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVgbz1IGLbTn"
      },
      "source": [
        "#### Model 4: Implement an Encoder-Decoder model (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgj43NvjKZDO"
      },
      "source": [
        "\n",
        "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train an encoder-decoder model on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # OPTIONAL: Implement\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# OPTIONAL: Train and Print prediction(s)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Z7VO2cNHSD"
      },
      "source": [
        "#### Finally, you can use all the models you built in this exercise to create a model that incorporates embedding and bidirectionality into one model. (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Mj9eDjMDwX"
      },
      "source": [
        "\n",
        "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "\n",
        "    return model\n",
        "# OPTIONAL: Train and Print prediction(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiXZThFkN07F"
      },
      "source": [
        "### Final Notes and TODOs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   In this exercise we focus on  learning different recurrent network architectures for machine translation, However we don't evaluate the models on a separate test set. To follow best machine learning practices, you can make use of the `sklearn.model_selecttion.train_test_split()` function to create separate training and test datasets. You can then retrain each of the models on the training set and evaluate the prediction accuracy using the hold out set. Observe how the best model performance might change.\n",
        "* If you don't have any idea about French (like me :D) you can use a translater like [google translat](https://translate.google.com/) to compare the prediction(s) from your models with the respective original English sentence(s).\n",
        "*   TODO: Finally, you can also try to train and predict your models on other language pairs. As a convenient example, you can use your models to translate from French to English since you already have the data available.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}