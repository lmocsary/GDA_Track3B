{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PyCharm (adams)",
      "language": "python",
      "name": "pycharm-feb95198"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "CNN_LiveCoding_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzkrAaeKIEg6"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/Ex12_CNN_manual/Ex12_manual_cnn.ipynb) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXhH_rxIEhG"
      },
      "source": [
        "Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mAFV-k8IEhV"
      },
      "source": [
        "## Convolutional layers for image processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6xJiN1EIEhV"
      },
      "source": [
        "Time series data has one dimension (earlier<>later), but image data is a good example of data that has two dimensions (left<>right, up<>down). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqWf57hjIEhV"
      },
      "source": [
        "import tensorflow.keras as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbVjze-0IEhV",
        "outputId": "23d6142c-960d-4b23-d360-442104df1013"
      },
      "source": [
        "# ready made datasets to play in Keras, load the data\n",
        "fashion_mnist = tf.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUlzfqsmIEhW"
      },
      "source": [
        "Let's understand the data, which is now 3D because we have $N$ observations!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE-su_3kIEhW",
        "outputId": "fbc00b24-bf10-49f9-8cd0-acd2ecca7927"
      },
      "source": [
        "train_images.shape\n",
        "#tiny images, 28*28"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEz8hBdH87PX",
        "outputId": "c2627cdf-9010-4291-945b-24d9b600a983"
      },
      "source": [
        "train_labels.shape\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eVpLujqGR11",
        "outputId": "2a501bde-5dcf-4fe8-bb9d-18fb058b8111"
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bscGVE1T9Ol_",
        "outputId": "e6924938-ca61-4f09-b44b-89fc8675fb2b"
      },
      "source": [
        "#10 different classes\n",
        "np.max(train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISfmsSodIEhW"
      },
      "source": [
        "Image data (like multivariate time series data) is multidimensional. Each observation here is a square greyscale image with 28 pixels on the horizontal and vertical.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbhutmHO9p_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3dd5cef-51ab-48aa-8487-492195948f65"
      },
      "source": [
        "train_images[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg6JDOTJIEhW",
        "outputId": "20d29f19-8adf-41ce-fca5-95a8c0173f0b"
      },
      "source": [
        "#grey scale images that encode the intensity\n",
        "train_images[0][:20,:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "XWZX8rScIEhW",
        "outputId": "12e9374c-b940-40d1-a36f-1b910e70c3c2"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0], cmap=\"Greys_r\", vmin = 0, vmax = 255)\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "#ankle boot for class 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8SmatvAIEhX"
      },
      "source": [
        "We have labels on the type of product, e.g. `9` ankle boot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WkRVp9NIEhX",
        "outputId": "0a71acad-7c34-4cab-8c80-310414700db9"
      },
      "source": [
        "train_labels[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyU0hnpzIEhe"
      },
      "source": [
        "##  Convolutional layers in keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVXDJ7LWIEhe"
      },
      "source": [
        "Now to apply our convolutions on a much larger scale! Here is an article illustrating when to use [2D vs 3D Convolutions](https://towardsdatascience.com/2d-or-3d-a-simple-comparison-of-convolutional-neural-networks-for-automatic-segmentation-of-625308f52aa7#:~:text=2D%20CNNs%20use%202D%20convolutional,map%20for%20a%20single%20slice.&text=2D%20CNNs%20predict%20segmentation%20maps,volumetric%20patch%20of%20a%20scan)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZrsg3q_IEhe"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo1GFK9KIEhe",
        "outputId": "90f51357-c667-43dd-ad64-55dbf5fbde69"
      },
      "source": [
        "#create model\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "nb_filters_first_cnn = 10\n",
        "nb_filters_second_cnn = 10\n",
        "nb_out_layer_neurons = 10 # defined by number of classes in classification task \n",
        "filter_size = 3\n",
        "#10 filters ,each filter has size 3, if kernel size specified that way, 3 is interpretat as 3*3\n",
        "model.add(Conv2D(nb_filters_first_cnn, kernel_size=filter_size, activation=\"relu\", input_shape=(28,28,1)))\n",
        "# kernel size: specifying the height and width of the 2D convolution filter. \n",
        "#Can be a single integer to specify the same value for all spatial dimensions\n",
        "\n",
        "model.add(Conv2D(nb_filters_second_cnn, kernel_size=filter_size, activation=\"relu\"))\n",
        "# 3d datastructure from output of prev conv layer, will flatten into 1dim vector\n",
        "model.add(Flatten())\n",
        "model.add(Dense(nb_out_layer_neurons, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 10)        910       \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 5760)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                57610     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 58,620\n",
            "Trainable params: 58,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW62XgBqIEhf"
      },
      "source": [
        "Make sure you understand the dimensionality at each stage of the model:\n",
        "\n",
        "1. Dimension of the input image (28,28,1) -> What is the 3rd dimension?\n",
        "2. Dimension of the second convolutional layer -> Why 24x24? Why 10 on the third axis?\n",
        "3. Why does the second convolutional layer have much more parameters than the first?\n",
        "4. Why the large number of parameters in the Dense layer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzfRfOq7IEhf"
      },
      "source": [
        "For convolutional layers, we can calculate the number of parameters as `(input_channels * filter_size* filter_size + 1)* output_channels`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkDycYdqIEhf",
        "outputId": "35cbc9a4-9eac-4890-dc60-db674c670fb9"
      },
      "source": [
        "(1  * (filter_size*filter_size) + 1) * nb_filters_first_cnn\n",
        "#first 1 replaced with 3 in case of coloured image with 3 channels/depth =3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlNWGNUk_kjN",
        "outputId": "116b7fa8-42c3-40e2-8179-63ab835b1b92"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0I_ODIvIEhg",
        "outputId": "880e0b03-7382-408c-efa1-9ca980a331c1"
      },
      "source": [
        "#filter covers full depth dimension of the input tensor! \n",
        "#first CONV LAyer gets as DATA the image, shape of image was 30*30*1, \n",
        "#we have a single colour channel in image bcs its grey scale\n",
        "#if it would be a colour image, we d have 3 colour channels, each would have values 0-255\n",
        "#filters cover full depth of input tensor\n",
        "#depth dimension of input image is 1 for first COnv LAyer. So the first COnv Layer gets 28*28*1\n",
        "#in 2nd Conv Layer, we used 10 filters in first COnv Layer, depth of output of 1st Conv Layer is 10 = nb of filters\n",
        "# here we have (3*3+1) *10\n",
        "(nb_filters_first_cnn * (filter_size*filter_size) + 1) * nb_filters_second_cnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "910"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7pAk0HaIEhg"
      },
      "source": [
        "When moving from a convolutional layer to a dense layer, we flatten the output of the former to comply with the specification of the input to a dense layer. That is the purpose of the *Flatten layer* in the above ConvNet. We can calculate the output dimension of the flatten layer as the product along all axes of the input tensor. In our case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0rmf2UAIEhg",
        "outputId": "294b49bd-cba7-4c1e-8117-0f7d21f6e8d6"
      },
      "source": [
        "flatten_layer_output = 24*24*nb_filters_second_cnn\n",
        "flatten_layer_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5760"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD-qxwcqIEhh"
      },
      "source": [
        "For the dense layer, which is also the output layer of our ConvNet, we obtain the number of parameters in the usual way as `(nb_inputs + 1)*nb_outputs`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urVVa0lTIEhh",
        "outputId": "2f65bd07-f2d7-4e08-f6ae-5e17ee03900d"
      },
      "source": [
        "nb_out_layer_neurons*(flatten_layer_output+1)\n",
        "#nb of units *(dim of inputs +1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57610"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVvDZ0GkIEhh"
      },
      "source": [
        "#### Max-Pooling  \n",
        "\n",
        "https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf976B4ECSL5"
      },
      "source": [
        "pooling layer operates upon each feature map separately to create a new set of the same number of pooled feature maps.  Two common functions used in the pooling operation are:\n",
        "\n",
        "Average Pooling: Calculate the average value for each patch on the feature map.\n",
        "\n",
        "Maximum Pooling (or Max Pooling): Calculate the maximum value for each patch of the feature map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpNVH6NnIEhh"
      },
      "source": [
        "#create model\n",
        "model = Sequential()\n",
        "\n",
        "# Add padding to keep the dimensionality of the 'hidden image' constant\n",
        "model.add(Conv2D(10, kernel_size=3, activation=\"relu\", input_shape=(28,28,1), padding=\"same\", name=\"conv1\"))\n",
        "# kernel size: specifying the height and width of the 2D convolution filter. \n",
        "#Can be a single integer to specify the same value for all spatial dimensions\n",
        "\n",
        "# Add a max-pooling layer to reduce the size of the image over which we convolute, define pool size. 2*2 is default, means dimension of input tensor is halfed!!!\n",
        "# no trainable parameters in POOLING LAYER\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# 2x2 pooling window, If only one integer is specified, the same window length will be used for both dimensions\n",
        "\n",
        "#pool_size: integer or tuple of 2 integers, window size over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window. If only one integer is specified, the same window length will be used for both dimensions.\n",
        "#strides: Integer, tuple of 2 integers, or None. \n",
        "#Strides values. Specifies how far the pooling window moves for each pooling step. \n",
        "#If None, it will default to pool_size.#\n",
        "\n",
        "model.add(Conv2D(10, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKJvMGAHIEhi",
        "outputId": "92edd2c4-9984-4816-b65f-7feebf6250fe"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1 (Conv2D)               (None, 28, 28, 10)        100       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 14, 14, 10)        910       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1960)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                19610     \n",
            "=================================================================\n",
            "Total params: 20,620\n",
            "Trainable params: 20,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSuXjG14IEhi"
      },
      "source": [
        "#### 1x1 Convolution\n",
        "\n",
        "A 1x1 is often used to reduce the number of depth channels, since it is often very slow to multiply volumes with extremely large depths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weywT3htIEhi"
      },
      "source": [
        "#create model\n",
        "model = Sequential()\n",
        "\n",
        "# Add padding to keep the dimensionality of the 'hidden image' constant\n",
        "#specify shape of input, with padding = same we ensure that image does not shrink, we want to have full control over size\n",
        "# pool size, specified\n",
        "model.add(Conv2D(10, kernel_size=3, activation=\"relu\", input_shape=(28,28,1), padding=\"same\", name=\"conv1\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(10, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2\"))\n",
        "\n",
        "# Add a 1x1 convolution to reduce the dimensionality of the Dense layer\n",
        "# by reducing the 'depth' of the hidden image\n",
        "# add conv layer with filter size is 1 and nb of filters is 1, ie. single weight\n",
        "# every filter goes over teh whole depth of input data!, ie 10 \n",
        "#here 1*1 conv is averaging over depth dimension of 10!! We are integrating over depth DImension\n",
        "model.add(Conv2D(1, kernel_size=1, activation=\"relu\", padding=\"same\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-68v_EhoIEhi",
        "outputId": "f663ca0d-a145-46f2-e0b6-0090a3f9b129"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1 (Conv2D)               (None, 28, 28, 10)        100       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 14, 14, 10)        910       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 1)         11        \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 196)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1970      \n",
            "=================================================================\n",
            "Total params: 2,991\n",
            "Trainable params: 2,991\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOp_k6szwasi"
      },
      "source": [
        "Thanks to 1*1 conv layer we get rid of depth dimension in our dense layer, therefore weight matrix is a lot smaller, need a lot fewer weights that can make training a lot easier. Note that the pooling layer has no trainable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HmnQApAIEhj"
      },
      "source": [
        "#### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aARsNNbIEhj"
      },
      "source": [
        "Finally, let's train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf2HmZdIEhj"
      },
      "source": [
        "#compile model using accuracy to measure model performance\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r5iwug9w9hK",
        "outputId": "0019b0e7-5eaa-45c1-fb70-31dfceee39d6"
      },
      "source": [
        "train_images.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBpyiZ8OIEhj"
      },
      "source": [
        "# Reshape the images to comply with tensorflow format (height x width x channel)\n",
        "train_images, test_images = train_images.reshape(train_images.shape[0],28,28,1), test_images.reshape(test_images.shape[0],28,28,1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hKs93E_iZru",
        "outputId": "84880210-06ac-41bb-e723-8bb95d04298b"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrSXqvYCIEhj",
        "outputId": "9d05238f-32a4-4f67-b57b-2418fea7c977"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIETFsH1IEhj"
      },
      "source": [
        "Preprocessing for image data is easier/standardized, because we know the technical minimum and maximum values that pixels can take."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrC1jvgTiiC8"
      },
      "source": [
        "train_images[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jazp1MVZ7B08",
        "outputId": "81824c3b-9d2f-4552-db73-b0aadc7c25d3"
      },
      "source": [
        "np.max(train_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEW2PCRXIEhk"
      },
      "source": [
        "#we scale data, scalung is trivial by 255, cause this is how images are stored\n",
        "train_images = train_images.astype(\"float32\") / 255.0\n",
        "test_images = test_images.astype(\"float32\") / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiqkutI3IEhk"
      },
      "source": [
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7bDF_tgIEhk"
      },
      "source": [
        "# one-hot encode the training and testing labels\n",
        "train_labels = np_utils.to_categorical(train_labels, 10)\n",
        "test_labels = np_utils.to_categorical(test_labels, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88T_lwv1IEhk",
        "outputId": "505efd8d-4079-4c92-cde3-14933dd1e3a0"
      },
      "source": [
        "test_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsa2_zwZIEhk",
        "outputId": "764f5bf4-8f60-4309-ca61-f4d12fa74faf"
      },
      "source": [
        "\n",
        "test_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6qlKHZnIEhl",
        "outputId": "d9be4280-126e-4600-a8fe-1d2c8231c53c"
      },
      "source": [
        "print(f'Loss and accuracy:\\n {model.evaluate(test_images, test_labels)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 8ms/step - loss: 2.3250 - accuracy: 0.0260\n",
            "Loss and accuracy:\n",
            " [2.3269565105438232, 0.025800000876188278]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idiqV8BtIEhl",
        "outputId": "c12d83c7-9361-45de-dc9b-9b080e3d2e29"
      },
      "source": [
        "story = model.fit(train_images, train_labels, validation_data=(test_images, test_labels), \n",
        "                  epochs=1, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 25s 53ms/step - loss: 0.8656 - accuracy: 0.6819 - val_loss: 0.6154 - val_accuracy: 0.7717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ztKlTsNIEhl",
        "outputId": "f8726701-1872-45ca-d5ca-f3e45b904e0e"
      },
      "source": [
        "print(f'Loss and accuracy:\\n {model.evaluate(test_images, test_labels)}')\n",
        "#evaluate on test set randomly initialized COnvNet, 82perc class accuracy from this 10 class problem, prety good"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 64us/step\n",
            "Loss and accuracy:\n",
            " [0.5059407576322555, 0.8210999965667725]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq_paU2BIEhl"
      },
      "source": [
        "## Further Reading: Pretrained Image Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0grgQT_IEhl"
      },
      "source": [
        "Check out https://keras.io/applications/  -> Documentation for individual models\n",
        "\n",
        "And Check out [Image Data Generators](https://towardsdatascience.com/keras-data-generators-and-how-to-use-them-b69129ed779c).\n",
        "\n",
        "[**Transferlearning**](https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e)\n",
        "\n",
        "\n",
        " Though training a CNN from scratch is possible for small projects, most applications require the training of very large CNNs and this as you guessed, takes extremely huge amounts of processed data and computational power. And both of these are not found so easily these days.\n",
        "Thats where transfer learning comes into play. In transfer learning, we take the pre-trained weights of an already trained model(one that has been trained on millions of images belonging to 1000s of classes, on several high power GPUs for several days) and use these already learned features to predict new classes.\n",
        "The advantages of transfer learning are that:\n",
        "\n",
        "1: There is no need of an extremely large training dataset.\n",
        "\n",
        "2: Not much computational power is required. As we are using pre-trained weights and only have to learn the weights of the last few layers.\n",
        "There are several models that have been trained on the image net dataset and have been open sourced.\n",
        "For example, VGG-16, VGG-19, Inception-V3 etc. For more details about each of these models, read the official keras documentation here.\n",
        " - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiyRvK-0IEhm"
      },
      "source": [
        "NOTE: Doesn't really work with fashion MNIST, since the images are too small (28x28) for state-of-the-art models trained on imagenet (starts at 128x128). ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. The project has been instrumental in advancing computer vision and deep learning research."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDq2IjaV5VBx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}